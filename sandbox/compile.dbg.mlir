// -----// IR Dump After AssignLegacyTargetDevicesPass (iree-hal-assign-legacy-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {hal.device.targets = [#device_target_local]} {
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputLegalityPass (iree-verify-input-legality) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AttributeCallGraphPass (iree-util-attribute-call-graph) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %8 = tensor.empty() : tensor<20x4096x4096xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %18 = arith.extf %in : f16 to f32
        %19 = arith.extf %in_1 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %out : f32
        linalg.yield %21 : f32
      } -> tensor<20x4096x4096xf32>
      %11 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %18 = arith.mulf %in, %cst_0 : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %12 = tensor.empty() : tensor<20x4096x64xf32>
      %13 = tensor.empty() : tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
        %18 = arith.addf %arg7, %arg10 : f32
        %19 = arith.truncf %arg7 : f32 to f16
        %20 = arith.extf %19 : f16 to f32
        %21 = arith.extf %arg8 : f16 to f32
        %22 = arith.mulf %20, %21 : f32
        %23 = arith.addf %22, %arg11 : f32
        iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After InitializeEmptyTensorsPass (iree-flow-initialize-empty-tensors) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
      (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %8 = tensor.empty() : tensor<20x4096x4096xf32>
    %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %18 = arith.extf %in : f16 to f32
      %19 = arith.extf %in_1 : f16 to f32
      %20 = arith.mulf %18, %19 : f32
      %21 = arith.addf %20, %out : f32
      linalg.yield %21 : f32
    } -> tensor<20x4096x4096xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.mulf %in, %cst_0 : f32
      linalg.yield %18 : f32
    } -> tensor<20x4096x4096xf32>
    %12 = tensor.empty() : tensor<20x4096x64xf32>
    %13 = tensor.empty() : tensor<20x4096xf32>
    %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
      %18 = arith.addf %arg7, %arg10 : f32
      %19 = arith.truncf %arg7 : f32 to f16
      %20 = arith.extf %19 : f16 to f32
      %21 = arith.extf %arg8 : f16 to f32
      %22 = arith.mulf %20, %21 : f32
      %23 = arith.addf %22, %arg11 : f32
      iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CaptureDynamicDimsPass (iree-flow-capture-dynamic-dims) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = hal.tensor.import %arg2 "input2" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
      (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x4096xf32>>, %arg6: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %7 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %8 = tensor.empty() : tensor<20x4096x4096xf32>
    %9 = linalg.fill ins(%cst : f32) outs(%8 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%5, %6 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%9 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %18 = arith.extf %in : f16 to f32
      %19 = arith.extf %in_1 : f16 to f32
      %20 = arith.mulf %18, %19 : f32
      %21 = arith.addf %20, %out : f32
      linalg.yield %21 : f32
    } -> tensor<20x4096x4096xf32>
    %11 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%10 : tensor<20x4096x4096xf32>) outs(%8 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %18 = arith.mulf %in, %cst_0 : f32
      linalg.yield %18 : f32
    } -> tensor<20x4096x4096xf32>
    %12 = tensor.empty() : tensor<20x4096x64xf32>
    %13 = tensor.empty() : tensor<20x4096xf32>
    %14 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %15 = linalg.fill ins(%cst : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %16 = linalg.fill ins(%cst_0 : f32) outs(%13 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %17:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%11, %7 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%15, %16, %14 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg7: f32, %arg8: f16, %arg9: f32, %arg10: f32, %arg11: f32):
      %18 = arith.addf %arg7, %arg10 : f32
      %19 = arith.truncf %arg7 : f32 to f16
      %20 = arith.extf %19 : f16 to f32
      %21 = arith.extf %arg8 : f16 to f32
      %22 = arith.mulf %20, %21 : f32
      %23 = arith.addf %22, %arg11 : f32
      iree_linalg_ext.yield %arg9, %18, %23 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %17#2, %arg6, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %4 = hal.tensor.export %3 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
      (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %4 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %5 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %7 = tensor.empty() : tensor<20x4096x4096xf32>
    %8 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%8 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %17 = arith.extf %in : f16 to f32
      %18 = arith.extf %in_1 : f16 to f32
      %19 = arith.mulf %17, %18 : f32
      %20 = arith.addf %19, %out : f32
      linalg.yield %20 : f32
    } -> tensor<20x4096x4096xf32>
    %10 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%9 : tensor<20x4096x4096xf32>) outs(%7 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %17 = arith.mulf %in, %cst_0 : f32
      linalg.yield %17 : f32
    } -> tensor<20x4096x4096xf32>
    %11 = tensor.empty() : tensor<20x4096x64xf32>
    %12 = tensor.empty() : tensor<20x4096xf32>
    %13 = linalg.fill ins(%cst_0 : f32) outs(%11 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %14 = linalg.fill ins(%cst : f32) outs(%12 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %15 = linalg.fill ins(%cst_0 : f32) outs(%12 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %16:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%10, %6 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%14, %15, %13 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
      %17 = arith.addf %arg6, %arg9 : f32
      %18 = arith.truncf %arg6 : f32 to f16
      %19 = arith.extf %18 : f16 to f32
      %20 = arith.extf %arg7 : f16 to f32
      %21 = arith.mulf %19, %20 : f32
      %22 = arith.addf %21, %arg10 : f32
      iree_linalg_ext.yield %arg8, %17, %22 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %16#2, %arg5, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch.workgroups(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
      (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %4 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %5 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %6 = tensor.empty() : tensor<20x4096x4096xf32>
    %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%7 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %16 = arith.extf %in : f16 to f32
      %17 = arith.extf %in_1 : f16 to f32
      %18 = arith.mulf %16, %17 : f32
      %19 = arith.addf %18, %out : f32
      linalg.yield %19 : f32
    } -> tensor<20x4096x4096xf32>
    %9 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%8 : tensor<20x4096x4096xf32>) outs(%6 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %16 = arith.mulf %in, %cst_0 : f32
      linalg.yield %16 : f32
    } -> tensor<20x4096x4096xf32>
    %10 = tensor.empty() : tensor<20x4096x64xf32>
    %11 = tensor.empty() : tensor<20x4096xf32>
    %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %13 = linalg.fill ins(%cst : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %14 = linalg.fill ins(%cst_0 : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %15:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%9, %5 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%13, %14, %12 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
      %16 = arith.addf %arg6, %arg9 : f32
      %17 = arith.truncf %arg6 : f32 to f16
      %18 = arith.extf %17 : f16 to f32
      %19 = arith.extf %arg7 : f16 to f32
      %20 = arith.mulf %18, %19 : f32
      %21 = arith.addf %20, %arg10 : f32
      iree_linalg_ext.yield %arg8, %16, %21 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %15#2, %arg5, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After OutlineDispatchExternsPass (iree-flow-outline-dispatch-externs) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch.workgroups(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
        (%arg3: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg5: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %4 = iree_tensor_ext.dispatch.tensor.load %arg3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %5 = iree_tensor_ext.dispatch.tensor.load %arg4, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %6 = tensor.empty() : tensor<20x4096x4096xf32>
      %7 = linalg.fill ins(%cst : f32) outs(%6 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %8 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%7 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %16 = arith.extf %in : f16 to f32
        %17 = arith.extf %in_1 : f16 to f32
        %18 = arith.mulf %16, %17 : f32
        %19 = arith.addf %18, %out : f32
        linalg.yield %19 : f32
      } -> tensor<20x4096x4096xf32>
      %9 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%8 : tensor<20x4096x4096xf32>) outs(%6 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %16 = arith.mulf %in, %cst_0 : f32
        linalg.yield %16 : f32
      } -> tensor<20x4096x4096xf32>
      %10 = tensor.empty() : tensor<20x4096x64xf32>
      %11 = tensor.empty() : tensor<20x4096xf32>
      %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %13 = linalg.fill ins(%cst : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %14 = linalg.fill ins(%cst_0 : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %15:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%9, %5 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%13, %14, %12 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
        %16 = arith.addf %arg6, %arg9 : f32
        %17 = arith.truncf %arg6 : f32 to f16
        %18 = arith.extf %17 : f16 to f32
        %19 = arith.extf %arg7 : f16 to f32
        %20 = arith.mulf %18, %19 : f32
        %21 = arith.addf %20, %arg10 : f32
        iree_linalg_ext.yield %arg8, %16, %21 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %15#2, %arg5, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegionsPass (iree-flow-outline-dispatch-regions) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchesPass (iree-flow-annotate-dispatches) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After StripDebugOpsPass (iree-util-strip-debug-ops) //----- //
flow.executable private @attention_dispatch_0 {
  flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %2 = tensor.empty() : tensor<20x4096x4096xf32>
      %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %4 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %12 = arith.extf %in : f16 to f32
        %13 = arith.extf %in_1 : f16 to f32
        %14 = arith.mulf %12, %13 : f32
        %15 = arith.addf %14, %out : f32
        linalg.yield %15 : f32
      } -> tensor<20x4096x4096xf32>
      %5 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %12 = arith.mulf %in, %cst_0 : f32
        linalg.yield %12 : f32
      } -> tensor<20x4096x4096xf32>
      %6 = tensor.empty() : tensor<20x4096x64xf32>
      %7 = tensor.empty() : tensor<20x4096xf32>
      %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
        %12 = arith.addf %arg3, %arg6 : f32
        %13 = arith.truncf %arg3 : f32 to f16
        %14 = arith.extf %13 : f16 to f32
        %15 = arith.extf %arg4 : f16 to f32
        %16 = arith.mulf %14, %15 : f32
        %17 = arith.addf %16, %arg7 : f32
        iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      return
    }
  }
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutablesPass (iree-flow-deduplicate-executables) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CleanupTensorShapesPass (iree-flow-cleanup-tensor-shapes) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After OutlineConstantsPass (iree-flow-outline-constants) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputPass (iree-stream-verify-input) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
  %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
  %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After CloneToConsumersPass (iree-stream-clone-to-consumers) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  flow.executable private @attention_dispatch_0 {
    flow.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %2 = tensor.empty() : tensor<20x4096x4096xf32>
        %3 = linalg.fill ins(%cst : f32) outs(%2 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %4 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%0, %1 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%3 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %12 = arith.extf %in : f16 to f32
          %13 = arith.extf %in_1 : f16 to f32
          %14 = arith.mulf %12, %13 : f32
          %15 = arith.addf %14, %out : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %5 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%4 : tensor<20x4096x4096xf32>) outs(%2 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %12 = arith.mulf %in, %cst_0 : f32
          linalg.yield %12 : f32
        } -> tensor<20x4096x4096xf32>
        %6 = tensor.empty() : tensor<20x4096x64xf32>
        %7 = tensor.empty() : tensor<20x4096xf32>
        %8 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %9 = linalg.fill ins(%cst : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %10 = linalg.fill ins(%cst_0 : f32) outs(%7 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %11:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%5, %1 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%9, %10, %8 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %12 = arith.addf %arg3, %arg6 : f32
          %13 = arith.truncf %arg3 : f32 to f16
          %14 = arith.extf %13 : f16 to f32
          %15 = arith.extf %arg4 : f16 to f32
          %16 = arith.mulf %14, %15 : f32
          %17 = arith.addf %16, %arg7 : f32
          iree_linalg_ext.yield %arg5, %12, %17 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %11#2, %arg2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %1 = hal.tensor.import %arg1 "input1" : !hal.buffer_view -> tensor<20x4096x64xf16>
    %2 = flow.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0, %1) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32>
    %3 = hal.tensor.export %2 "output0" : tensor<20x4096x64xf32> -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStreamPass (iree-stream-conversion) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c20 = arith.constant 20 : index
    %c4096 = arith.constant 4096 : index
    %c64 = arith.constant 64 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c20_2 = arith.constant 20 : index
    %c4096_3 = arith.constant 4096 : index
    %c64_4 = arith.constant 64 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20_2, %c4096_3, %c64_4]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %7 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %5) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%3}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<20x4096x64xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensorsPass (iree-stream-verify-lowering-to-tensors) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c20 = arith.constant 20 : index
    %c4096 = arith.constant 4096 : index
    %c64 = arith.constant 64 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    %c20_2 = arith.constant 20 : index
    %c4096_3 = arith.constant 4096 : index
    %c64_4 = arith.constant 64 : index
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20_2, %c4096_3, %c64_4]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%3}
    %5 = stream.async.transfer %4 : !stream.resource<external>{%3} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %7 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %5) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%3}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%6}
    %8 = stream.async.transfer %7 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<20x4096x64xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
  %cst = arith.constant 1.250000e-01 : f32
  %cst_0 = arith.constant 0.000000e+00 : f32
  %c0 = arith.constant 0 : index
  %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
    %15 = arith.addf %arg3, %arg6 : f32
    %16 = arith.truncf %arg3 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg4 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg7 : f32
    iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %element_type_f16_0 = hal.element_type<f16> : i32
  %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16_0) encoding(%dense_row_major_1)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%3}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %7 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %5) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%3}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%6}
  %8 = stream.async.clone on(#hal.device.affinity<@__device_0>) %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<20x4096x64xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %9 : !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %element_type_f16_0 = hal.element_type<f16> : i32
    %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16_0) encoding(%dense_row_major_1)
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%3}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %7 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %5) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%3}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%6}
    %8 = stream.async.clone on(#hal.device.affinity<@__device_0>) %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<20x4096x64xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
    util.return %9 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %element_type_f16_0 = hal.element_type<f16> : i32
  %dense_row_major_1 = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16_0) encoding(%dense_row_major_1)
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %4 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%3}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<external>{%3} -> !stream.resource<*>{%3}
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %7 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %5) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%3}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%6}
  %8 = stream.async.clone on(#hal.device.affinity<@__device_0>) %7 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<20x4096x64xf32> in !stream.resource<external>{%6} -> !hal.buffer_view
  util.return %9 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializersPass (iree-util-combine-initializers) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After SpecializeEncodingsPass (iree-stream-specialize-encodings) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
    %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
    %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
    util.return %8 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After EncodeDeviceTensorsPass (iree-stream-encode-device-tensors) //----- //
stream.executable private @attention_dispatch_0 {
  stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %cst = arith.constant 1.250000e-01 : f32
      %cst_0 = arith.constant 0.000000e+00 : f32
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %5 = tensor.empty() : tensor<20x4096x4096xf32>
      %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %15 = arith.extf %in : f16 to f32
        %16 = arith.extf %in_1 : f16 to f32
        %17 = arith.mulf %15, %16 : f32
        %18 = arith.addf %17, %out : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.mulf %in, %cst : f32
        linalg.yield %15 : f32
      } -> tensor<20x4096x4096xf32>
      %9 = tensor.empty() : tensor<20x4096x64xf32>
      %10 = tensor.empty() : tensor<20x4096xf32>
      %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
        %15 = arith.addf %arg3, %arg6 : f32
        %16 = arith.truncf %arg3 : f32 to f16
        %17 = arith.extf %16 : f16 to f32
        %18 = arith.extf %arg4 : f16 to f32
        %19 = arith.mulf %17, %18 : f32
        %20 = arith.addf %19, %arg7 : f32
        iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      return
    }
  }
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %3 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%0}
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %5 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<20x4096x64xf32> : index
  %6 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%2, %4) : (tensor<20x4096x64xf16> in !stream.resource<*>{%0}, tensor<20x4096x64xf16> in !stream.resource<*>{%0}) -> tensor<20x4096x64xf32> in !stream.resource<*>{%5}
  %7 = stream.async.clone on(#hal.device.affinity<@__device_0>) %6 : !stream.resource<*>{%5} -> !stream.resource<external>{%5}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<20x4096x64xf32> in !stream.resource<external>{%5} -> !hal.buffer_view
  util.return %8 : !hal.buffer_view
}

// -----// IR Dump After EncodeHostTensorsPass (iree-stream-encode-host-tensors) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After MaterializeEncodingsPass (iree-stream-materialize-encodings) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncResourcesPass (iree-stream-verify-lowering-to-async-resources) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWritePass (iree-stream-materialize-copy-on-write) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocationsPass (iree-stream-emplace-allocations) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<*>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<*>{%c10485760}, !stream.resource<*>{%c10485760}) -> !stream.resource<*>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<*>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After RefineUsagePass (iree-stream-refine-usage) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<external>{%c10485760}
    %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<external>{%c20971520} -> !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c10485760} -> !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %2 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %3 = stream.async.clone on(#hal.device.affinity<@__device_0>) %2 : !stream.resource<external>{%c10485760} -> !stream.resource<external>{%c10485760}
  %4 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%1[%c0 to %c10485760 for %c10485760], %3[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %5 = stream.async.clone on(#hal.device.affinity<@__device_0>) %4 : !stream.resource<external>{%c20971520} -> !stream.resource<external>{%c20971520}
  %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %6 : !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRangesPass (iree-stream-verify-async-access-ranges) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%0[%c0 to %c10485760 for %c10485760], %1[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecutionPass (iree-stream-schedule-execution) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrencyPass (iree-stream-schedule-concurrency) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SyncInitializersPass (iree-stream-sync-initializers) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %4 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After PropagateTimepointsPass (iree-stream-propagate-timepoints) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.timepoint.immediate => !stream.timepoint
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.join max(%2, %3) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%4) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %7 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %7 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %5 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeBuiltinsPass (iree-stream-materialize-builtins) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %2 = stream.timepoint.immediate => !stream.timepoint
    %3 = stream.timepoint.immediate => !stream.timepoint
    %4 = stream.timepoint.join max(%2, %3) => !stream.timepoint
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%4) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %7 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %7 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %5 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %6 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %6 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
    %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
    stream.yield %4 : !stream.resource<external>{%c20971520}
  } => !stream.timepoint
  %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %3 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %4 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %4 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %4 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncPass (iree-stream-verify-lowering-to-async) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %results, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520} {
      %4 = stream.async.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg3[%c0 to %c10485760 for %c10485760], %arg4[%c0 to %c10485760 for %c10485760]) : (!stream.resource<external>{%c10485760}, !stream.resource<external>{%c10485760}) -> !stream.resource<external>{%c20971520}
      stream.yield %4 : !stream.resource<external>{%c20971520}
    } => !stream.timepoint
    %2 = stream.timepoint.await %result_timepoint => %results : !stream.resource<external>{%c20971520}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %3 : !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocationPass (iree-stream-schedule-allocation) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After EmplaceTransientsPass (iree-stream-emplace-transients) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTransientSizeQueriesPass (iree-stream-materialize-transient-size-queries) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %c0_0 = arith.constant 0 : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PackConstantsPass (iree-stream-pack-constants) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %c0_0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After LayoutSlicesPass (iree-stream-layout-slices) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %c0_0 = arith.constant 0 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After PropagateSubrangesPass (iree-util-propagate-subranges) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AutomaticReferenceCountingPass (iree-stream-automatic-reference-counting) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateConstantTransientSizePass (iree-stream-annotate-constant-transient-size) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmdPass (iree-stream-verify-lowering-to-cmd) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlowPass (convert-scf-to-cf) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ReuseAllocationsPass (iree-stream-reuse-allocations) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepointsPass (iree-stream-elide-timepoints) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindingsPass (iree-stream-fuse-dispatch-bindings) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
          %15 = arith.addf %arg6, %arg9 : f32
          %16 = arith.truncf %arg6 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg7 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg10 : f32
          iree_linalg_ext.yield %arg8, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArgumentsPass (iree-stream-annotate-dispatch-arguments) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst_0 : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
          %15 = arith.addf %arg6, %arg9 : f32
          %16 = arith.truncf %arg6 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg7 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg10 : f32
          iree_linalg_ext.yield %arg8, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchAssumptionsPass (iree-stream-annotate-dispatch-assumptions) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.values = [0 : index]}) {
        %0:3 = util.assume.int
            %arg3<umin = 0, umax = 0>,
            %arg4<umin = 0, umax = 0>,
            %arg5<umin = 0, umax = 0>
          : index, index, index
        %cst = arith.constant 1.250000e-01 : f32
        %cst_0 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %1 = stream.binding.subspan %arg0[%0#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg1[%0#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %3 = stream.binding.subspan %arg2[%0#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %6 = tensor.empty() : tensor<20x4096x4096xf32>
        %7 = linalg.fill ins(%cst_0 : f32) outs(%6 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%7 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %16 = arith.extf %in : f16 to f32
          %17 = arith.extf %in_1 : f16 to f32
          %18 = arith.mulf %16, %17 : f32
          %19 = arith.addf %18, %out : f32
          linalg.yield %19 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%8 : tensor<20x4096x4096xf32>) outs(%6 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %16 = arith.mulf %in, %cst : f32
          linalg.yield %16 : f32
        } -> tensor<20x4096x4096xf32>
        %10 = tensor.empty() : tensor<20x4096x64xf32>
        %11 = tensor.empty() : tensor<20x4096xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14 = linalg.fill ins(%cst : f32) outs(%11 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %15:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%9, %5 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%13, %14, %12 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg6: f32, %arg7: f16, %arg8: f32, %arg9: f32, %arg10: f32):
          %16 = arith.addf %arg6, %arg9 : f32
          %17 = arith.truncf %arg6 : f32 to f16
          %18 = arith.extf %17 : f16 to f32
          %19 = arith.extf %arg7 : f16 to f32
          %20 = arith.mulf %18, %19 : f32
          %21 = arith.addf %20, %arg10 : f32
          iree_linalg_ext.yield %arg8, %16, %21 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %15#2, %3, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0, %c0, %c0 : index, index, index) {
        ro %arg3[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperandsPass (iree-stream-pack-dispatch-operands) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c32_i64 = arith.constant 32 : i64
        %0 = arith.extui %arg4 : i32 to i64
        %1 = arith.shli %0, %c32_i64 : i64
        %2 = arith.extui %arg3 : i32 to i64
        %3 = arith.ori %2, %1 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %c32_i64_0 = arith.constant 32 : i64
        %5 = arith.extui %arg6 : i32 to i64
        %6 = arith.shli %5, %c32_i64_0 : i64
        %7 = arith.extui %arg5 : i32 to i64
        %8 = arith.ori %7, %6 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %c32_i64_1 = arith.constant 32 : i64
        %10 = arith.extui %arg8 : i32 to i64
        %11 = arith.shli %10, %c32_i64_1 : i64
        %12 = arith.extui %arg7 : i32 to i64
        %13 = arith.ori %12, %11 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15:3 = util.assume.int
            %4<umin = 0, umax = 0>,
            %9<umin = 0, umax = 0>,
            %14<umin = 0, umax = 0>
          : index, index, index
        %cst = arith.constant 1.250000e-01 : f32
        %cst_2 = arith.constant 0.000000e+00 : f32
        %c0 = arith.constant 0 : index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %19 = iree_tensor_ext.dispatch.tensor.load %16, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %20 = iree_tensor_ext.dispatch.tensor.load %17, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %21 = tensor.empty() : tensor<20x4096x4096xf32>
        %22 = linalg.fill ins(%cst_2 : f32) outs(%21 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %23 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%19, %20 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%22 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_3: f16, %out: f32):
          %31 = arith.extf %in : f16 to f32
          %32 = arith.extf %in_3 : f16 to f32
          %33 = arith.mulf %31, %32 : f32
          %34 = arith.addf %33, %out : f32
          linalg.yield %34 : f32
        } -> tensor<20x4096x4096xf32>
        %24 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%23 : tensor<20x4096x4096xf32>) outs(%21 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %31 = arith.mulf %in, %cst : f32
          linalg.yield %31 : f32
        } -> tensor<20x4096x4096xf32>
        %25 = tensor.empty() : tensor<20x4096x64xf32>
        %26 = tensor.empty() : tensor<20x4096xf32>
        %27 = linalg.fill ins(%cst : f32) outs(%25 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %28 = linalg.fill ins(%cst_2 : f32) outs(%26 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %29 = linalg.fill ins(%cst : f32) outs(%26 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %30:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%24, %20 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%28, %29, %27 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg9: f32, %arg10: f16, %arg11: f32, %arg12: f32, %arg13: f32):
          %31 = arith.addf %arg9, %arg12 : f32
          %32 = arith.truncf %arg9 : f32 to f16
          %33 = arith.extf %32 : f16 to f32
          %34 = arith.extf %arg10 : f16 to f32
          %35 = arith.mulf %33, %34 : f32
          %36 = arith.addf %35, %arg13 : f32
          iree_linalg_ext.yield %arg11, %31, %36 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %30#2, %18, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %c0_0 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c32_i64 = arith.constant 32 : i64
    %c0_i64_1 = arith.constant 0 : i64
    %c0_i32_2 = arith.constant 0 : i32
    %c0_i64_3 = arith.constant 0 : i64
    %c0_i32_4 = arith.constant 0 : i32
    %c32_i64_5 = arith.constant 32 : i64
    %c0_i64_6 = arith.constant 0 : i64
    %c0_i32_7 = arith.constant 0 : i32
    %c0_i64_8 = arith.constant 0 : i64
    %c0_i32_9 = arith.constant 0 : i32
    %c32_i64_10 = arith.constant 32 : i64
    %c0_i64_11 = arith.constant 0 : i64
    %c0_i32_12 = arith.constant 0 : i32
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32_2, %c0_i32_4, %c0_i32_7, %c0_i32_9, %c0_i32_12 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0_0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0_0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg9: f32, %arg10: f16, %arg11: f32, %arg12: f32, %arg13: f32):
          %15 = arith.addf %arg9, %arg12 : f32
          %16 = arith.truncf %arg9 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg10 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg13 : f32
          iree_linalg_ext.yield %arg11, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg9: f32, %arg10: f16, %arg11: f32, %arg12: f32, %arg13: f32):
          %15 = arith.addf %arg9, %arg12 : f32
          %16 = arith.truncf %arg9 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg10 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg13 : f32
          iree_linalg_ext.yield %arg11, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg9: f32, %arg10: f16, %arg11: f32, %arg12: f32, %arg13: f32):
          %15 = arith.addf %arg9, %arg12 : f32
          %16 = arith.truncf %arg9 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg10 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg13 : f32
          iree_linalg_ext.yield %arg11, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperandsPass (iree-stream-fold-uniform-operands) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AttributeCallGraphPass (iree-util-attribute-call-graph) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignLegacyTargetDevicesPass (iree-hal-assign-legacy-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
  %c0 = arith.constant 0 : index
  %c20971520 = arith.constant 20971520 : index
  %c10485760 = arith.constant 10485760 : index
  %c64 = arith.constant 64 : index
  %c4096 = arith.constant 4096 : index
  %c20 = arith.constant 20 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
    stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
      ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
      wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
    }
  } => !stream.timepoint
  %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
  util.return %4 : !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  stream.executable private @attention_dispatch_0 {
    stream.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg3: f32, %arg4: f16, %arg5: f32, %arg6: f32, %arg7: f32):
          %15 = arith.addf %arg3, %arg6 : f32
          %16 = arith.truncf %arg3 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg4 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg7 : f32
          iree_linalg_ext.yield %arg5, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  hal.executable private @attention_dispatch_0 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
          %cst = arith.constant 0.000000e+00 : f32
          %cst_0 = arith.constant 1.250000e-01 : f32
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
          %5 = tensor.empty() : tensor<20x4096x4096xf32>
          %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
          %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
          ^bb0(%in: f16, %in_1: f16, %out: f32):
            %15 = arith.extf %in : f16 to f32
            %16 = arith.extf %in_1 : f16 to f32
            %17 = arith.mulf %15, %16 : f32
            %18 = arith.addf %17, %out : f32
            linalg.yield %18 : f32
          } -> tensor<20x4096x4096xf32>
          %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
          ^bb0(%in: f32, %out: f32):
            %15 = arith.mulf %in, %cst_0 : f32
            linalg.yield %15 : f32
          } -> tensor<20x4096x4096xf32>
          %9 = tensor.empty() : tensor<20x4096x64xf32>
          %10 = tensor.empty() : tensor<20x4096xf32>
          %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
          %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
          %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
          ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
            %15 = arith.addf %arg0, %arg3 : f32
            %16 = arith.truncf %arg0 : f32 to f16
            %17 = arith.extf %16 : f16 to f32
            %18 = arith.extf %arg1 : f16 to f32
            %19 = arith.mulf %17, %18 : f32
            %20 = arith.addf %19, %arg4 : f32
            iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
          } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
          iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
          return
        }
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@embedded_elf_x86_64::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>
#config1 = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>
#executable_target_embedded_elf_x86_64 = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>
#map3 = affine_map<(d0, d1, d2) -> (d0, d1, d2)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1)>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_local = #hal.device.target<"local", [#executable_target_embedded_elf_x86_64]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_local
  hal.executable private @attention_dispatch_0 {
    hal.executable.variant public @embedded_elf_x86_64 target(#executable_target_embedded_elf_x86_64) {
      hal.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
          %cst = arith.constant 0.000000e+00 : f32
          %cst_0 = arith.constant 1.250000e-01 : f32
          %c0 = arith.constant 0 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
          %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
          %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
          %5 = tensor.empty() : tensor<20x4096x4096xf32>
          %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
          %7 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #config} {
          ^bb0(%in: f16, %in_1: f16, %out: f32):
            %15 = arith.extf %in : f16 to f32
            %16 = arith.extf %in_1 : f16 to f32
            %17 = arith.mulf %15, %16 : f32
            %18 = arith.addf %17, %out : f32
            linalg.yield %18 : f32
          } -> tensor<20x4096x4096xf32>
          %8 = linalg.generic {indexing_maps = [#map3, #map3], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
          ^bb0(%in: f32, %out: f32):
            %15 = arith.mulf %in, %cst_0 : f32
            linalg.yield %15 : f32
          } -> tensor<20x4096x4096xf32>
          %9 = tensor.empty() : tensor<20x4096x64xf32>
          %10 = tensor.empty() : tensor<20x4096xf32>
          %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
          %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
          %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
          %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [#map, #map4, #map5, #map5, #map2], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #config1} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
          ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
            %15 = arith.addf %arg0, %arg3 : f32
            %16 = arith.truncf %arg0 : f32 to f16
            %17 = arith.extf %16 : f16 to f32
            %18 = arith.extf %arg1 : f16 to f32
            %19 = arith.mulf %17, %18 : f32
            %20 = arith.addf %19, %arg4 : f32
            iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
          } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
          iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
          return
        }
      }
    }
  }
  util.func public @attention(%arg0: !hal.buffer_view, %arg1: !hal.buffer_view, %arg2: !hal.buffer_view) -> !hal.buffer_view attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @attention(%input0: tensor<20x4096x64xf16>, %input1: tensor<20x4096x64xf16>, %input2: tensor<20x4096x64xf16>) -> (%output0: tensor<20x4096x64xf32>)"}} {
    %c0 = arith.constant 0 : index
    %c20971520 = arith.constant 20971520 : index
    %c10485760 = arith.constant 10485760 : index
    %c64 = arith.constant 64 : index
    %c4096 = arith.constant 4096 : index
    %c20 = arith.constant 20 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    hal.buffer_view.assert<%arg1 : !hal.buffer_view> message("input1") shape([%c20, %c4096, %c64]) type(%element_type_f16) encoding(%dense_row_major)
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg1 : !hal.buffer_view -> tensor<20x4096x64xf16> in !stream.resource<external>{%c10485760}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c20971520} => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%result_timepoint) => with(%0 as %arg3: !stream.resource<external>{%c10485760}, %1 as %arg4: !stream.resource<external>{%c10485760}, %result as %arg5: !stream.resource<external>{%c20971520}) {
      stream.cmd.dispatch @attention_dispatch_0::@embedded_elf_x86_64::@attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 {
        ro %arg3[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        ro %arg4[%c0 for %c10485760] : !stream.resource<external>{%c10485760},
        wo %arg5[%c0 for %c20971520] : !stream.resource<external>{%c20971520}
      }
    } => !stream.timepoint
    %3 = stream.timepoint.await %2 => %result : !stream.resource<external>{%c20971520}
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %3 : tensor<20x4096x64xf32> in !stream.resource<external>{%c20971520} -> !hal.buffer_view
    util.return %4 : !hal.buffer_view
  }
}


// -----// IR Dump After SpecializeExportsPass (iree-codegen-specialize-exports) //----- //
hal.executable.variant public @embedded_elf_x86_64 target(<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %5 = tensor.empty() : tensor<20x4096x4096xf32>
      %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %15 = arith.extf %in : f16 to f32
        %16 = arith.extf %in_1 : f16 to f32
        %17 = arith.mulf %15, %16 : f32
        %18 = arith.addf %17, %out : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.mulf %in, %cst_0 : f32
        linalg.yield %15 : f32
      } -> tensor<20x4096x4096xf32>
      %9 = tensor.empty() : tensor<20x4096x64xf32>
      %10 = tensor.empty() : tensor<20x4096xf32>
      %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
        %15 = arith.addf %arg0, %arg3 : f32
        %16 = arith.truncf %arg0 : f32 to f16
        %17 = arith.extf %16 : f16 to f32
        %18 = arith.extf %arg1 : f16 to f32
        %19 = arith.mulf %17, %18 : f32
        %20 = arith.addf %19, %arg4 : f32
        iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      return
    }
  }
}

// -----// IR Dump After TypePropagationPass (iree-codegen-type-propagation) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOpsPass (iree-codegen-bubble-up-ordinal-ops) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatchesPass (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After DecomposeSoftmaxPass (iree-codegen-decompose-softmax) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After MaterializeUserConfigsPass (iree-codegen-materialize-user-configs) //----- //
module {
  func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %5 = tensor.empty() : tensor<20x4096x4096xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %15 = arith.extf %in : f16 to f32
      %16 = arith.extf %in_1 : f16 to f32
      %17 = arith.mulf %15, %16 : f32
      %18 = arith.addf %17, %out : f32
      linalg.yield %18 : f32
    } -> tensor<20x4096x4096xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %15 = arith.mulf %in, %cst_0 : f32
      linalg.yield %15 : f32
    } -> tensor<20x4096x4096xf32>
    %9 = tensor.empty() : tensor<20x4096x64xf32>
    %10 = tensor.empty() : tensor<20x4096xf32>
    %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
      %15 = arith.addf %arg0, %arg3 : f32
      %16 = arith.truncf %arg0 : f32 to f16
      %17 = arith.extf %16 : f16 to f32
      %18 = arith.extf %arg1 : f16 to f32
      %19 = arith.mulf %17, %18 : f32
      %20 = arith.addf %19, %arg4 : f32
      iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    return
  }
}

// -----// IR Dump After MaterializeDeviceEncodingPass (iree-codegen-materialize-device-encoding) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After CPUPropagateDataLayoutPass (iree-codegen-cpu-propagate-data-layout) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After RematerializeParallelOpsPass (iree-codegen-rematerialize-parallel-ops) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After ExpandF16OpToF32Pass (iree-llvmcpu-expand-f16-op-to-f32) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After ConvertAccGEMMToGEMMPass (iree-convert-accgemm-to-gemm) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

// -----// IR Dump After EraseHALDescriptorTypeFromMemRefPass (iree-codegen-erase-hal-descriptor-type-from-memref) //----- //
func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
  %cst = arith.constant 0.000000e+00 : f32
  %cst_0 = arith.constant 1.250000e-01 : f32
  %c0 = arith.constant 0 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
  %5 = tensor.empty() : tensor<20x4096x4096xf32>
  %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
  ^bb0(%in: f16, %in_1: f16, %out: f32):
    %15 = arith.extf %in : f16 to f32
    %16 = arith.extf %in_1 : f16 to f32
    %17 = arith.mulf %15, %16 : f32
    %18 = arith.addf %17, %out : f32
    linalg.yield %18 : f32
  } -> tensor<20x4096x4096xf32>
  %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
  ^bb0(%in: f32, %out: f32):
    %15 = arith.mulf %in, %cst_0 : f32
    linalg.yield %15 : f32
  } -> tensor<20x4096x4096xf32>
  %9 = tensor.empty() : tensor<20x4096x64xf32>
  %10 = tensor.empty() : tensor<20x4096xf32>
  %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %15 = arith.addf %arg0, %arg3 : f32
    %16 = arith.truncf %arg0 : f32 to f16
    %17 = arith.extf %16 : f16 to f32
    %18 = arith.extf %arg1 : f16 to f32
    %19 = arith.mulf %17, %18 : f32
    %20 = arith.addf %19, %arg4 : f32
    iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
  } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
  iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  return
}

./sandbox/dispatch.mlir:6:10: error: 'func.func' op failed to set lowering configuration
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
         ^
./sandbox/dispatch.mlir:6:10: note: see current operation:
"func.func"() <{function_type = () -> (), sym_name = "attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32"}> ({
  %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
  %1 = "arith.constant"() <{value = 1.250000e-01 : f32}> : () -> f32
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %4 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
  %5 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
  %6 = "iree_tensor_ext.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
  %7 = "iree_tensor_ext.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
  %8 = "tensor.empty"() : () -> tensor<20x4096x4096xf32>
  %9 = "linalg.fill"(%0, %8) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg16: f32, %arg17: f32):
    "linalg.yield"(%arg16) : (f32) -> ()
  }) : (f32, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %10 = "linalg.generic"(%6, %7, %9) <{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 1>}> ({
  ^bb0(%arg13: f16, %arg14: f16, %arg15: f32):
    %25 = "arith.extf"(%arg13) : (f16) -> f32
    %26 = "arith.extf"(%arg14) : (f16) -> f32
    %27 = "arith.mulf"(%25, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %28 = "arith.addf"(%27, %arg15) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "linalg.yield"(%28) : (f32) -> ()
  }) {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %11 = "linalg.generic"(%10, %8) <{indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg11: f32, %arg12: f32):
    %24 = "arith.mulf"(%arg11, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "linalg.yield"(%24) : (f32) -> ()
  }) : (tensor<20x4096x4096xf32>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
  %12 = "tensor.empty"() : () -> tensor<20x4096x64xf32>
  %13 = "tensor.empty"() : () -> tensor<20x4096xf32>
  %14 = "linalg.fill"(%1, %12) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg9: f32, %arg10: f32):
    "linalg.yield"(%arg9) : (f32) -> ()
  }) : (f32, tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
  %15 = "linalg.fill"(%0, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg7: f32, %arg8: f32):
    "linalg.yield"(%arg7) : (f32) -> ()
  }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %16 = "linalg.fill"(%1, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
  ^bb0(%arg5: f32, %arg6: f32):
    "linalg.yield"(%arg5) : (f32) -> ()
  }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
  %17:3 = "iree_linalg_ext.exp_reduction"(%11, %7, %15, %16, %14) <{exp_reduced_operands = array<i64: 1, 2>, indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 3>}> ({
  ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
    %18 = "arith.addf"(%arg0, %arg3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %19 = "arith.truncf"(%arg0) : (f32) -> f16
    %20 = "arith.extf"(%19) : (f16) -> f32
    %21 = "arith.extf"(%arg1) : (f16) -> f32
    %22 = "arith.mulf"(%20, %21) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    %23 = "arith.addf"(%22, %arg4) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
    "iree_linalg_ext.yield"(%arg2, %18, %23) : (f32, f32, f32) -> ()
  }) {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} : (tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>, tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) -> (tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>)
  "iree_tensor_ext.dispatch.tensor.store"(%17#2, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (tensor<20x4096x64xf32>, !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) -> ()
  "func.return"() : () -> ()
}) : () -> ()
// -----// IR Dump After LLVMCPUSelectLoweringStrategyPass Failed (iree-llvmcpu-select-lowering-strategy) //----- //
module {
  func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
    %cst = arith.constant 0.000000e+00 : f32
    %cst_0 = arith.constant 1.250000e-01 : f32
    %c0 = arith.constant 0 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
    %5 = tensor.empty() : tensor<20x4096x4096xf32>
    %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
    %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
    ^bb0(%in: f16, %in_1: f16, %out: f32):
      %15 = arith.extf %in : f16 to f32
      %16 = arith.extf %in_1 : f16 to f32
      %17 = arith.mulf %15, %16 : f32
      %18 = arith.addf %17, %out : f32
      linalg.yield %18 : f32
    } -> tensor<20x4096x4096xf32>
    %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
    ^bb0(%in: f32, %out: f32):
      %15 = arith.mulf %in, %cst_0 : f32
      linalg.yield %15 : f32
    } -> tensor<20x4096x4096xf32>
    %9 = tensor.empty() : tensor<20x4096x64xf32>
    %10 = tensor.empty() : tensor<20x4096xf32>
    %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
    %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
    %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
    ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
      %15 = arith.addf %arg0, %arg3 : f32
      %16 = arith.truncf %arg0 : f32 to f16
      %17 = arith.extf %16 : f16 to f32
      %18 = arith.extf %arg1 : f16 to f32
      %19 = arith.mulf %17, %18 : f32
      %20 = arith.addf %19, %arg4 : f32
      iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
    } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
    iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
    return
  }
}

./sandbox/dispatch.mlir:6:10: error: failed to run configuration of source executable to target executable for backend #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
         ^
./sandbox/dispatch.mlir:6:10: note: see current operation:
"hal.executable.variant"() <{sym_name = "embedded_elf_x86_64", target = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>}> ({
  "hal.executable.export"() <{layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, ordinal = 0 : index, sym_name = "attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32"}> ({
  ^bb0(%arg18: !hal.device):
    %29:3 = "iree_tensor_ext.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
    "hal.return"(%29#0, %29#1, %29#2) : (index, index, index) -> ()
  }, {
  }) : () -> ()
  "builtin.module"() ({
    "func.func"() <{function_type = () -> (), sym_name = "attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32"}> ({
      %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
      %1 = "arith.constant"() <{value = 1.250000e-01 : f32}> : () -> f32
      %2 = "arith.constant"() <{value = 0 : index}> : () -> index
      %3 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %4 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %5 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      %6 = "iree_tensor_ext.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
      %7 = "iree_tensor_ext.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
      %8 = "tensor.empty"() : () -> tensor<20x4096x4096xf32>
      %9 = "linalg.fill"(%0, %8) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg16: f32, %arg17: f32):
        "linalg.yield"(%arg16) : (f32) -> ()
      }) : (f32, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %10 = "linalg.generic"(%6, %7, %9) <{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 1>}> ({
      ^bb0(%arg13: f16, %arg14: f16, %arg15: f32):
        %25 = "arith.extf"(%arg13) : (f16) -> f32
        %26 = "arith.extf"(%arg14) : (f16) -> f32
        %27 = "arith.mulf"(%25, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %28 = "arith.addf"(%27, %arg15) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%28) : (f32) -> ()
      }) {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %11 = "linalg.generic"(%10, %8) <{indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg11: f32, %arg12: f32):
        %24 = "arith.mulf"(%arg11, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "linalg.yield"(%24) : (f32) -> ()
      }) : (tensor<20x4096x4096xf32>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %12 = "tensor.empty"() : () -> tensor<20x4096x64xf32>
      %13 = "tensor.empty"() : () -> tensor<20x4096xf32>
      %14 = "linalg.fill"(%1, %12) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg9: f32, %arg10: f32):
        "linalg.yield"(%arg9) : (f32) -> ()
      }) : (f32, tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %15 = "linalg.fill"(%0, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg7: f32, %arg8: f32):
        "linalg.yield"(%arg7) : (f32) -> ()
      }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %16 = "linalg.fill"(%1, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg5: f32, %arg6: f32):
        "linalg.yield"(%arg5) : (f32) -> ()
      }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %17:3 = "iree_linalg_ext.exp_reduction"(%11, %7, %15, %16, %14) <{exp_reduced_operands = array<i64: 1, 2>, indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 3>}> ({
      ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
        %18 = "arith.addf"(%arg0, %arg3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %19 = "arith.truncf"(%arg0) : (f32) -> f16
        %20 = "arith.extf"(%19) : (f16) -> f32
        %21 = "arith.extf"(%arg1) : (f16) -> f32
        %22 = "arith.mulf"(%20, %21) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        %23 = "arith.addf"(%22, %arg4) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
        "iree_linalg_ext.yield"(%arg2, %18, %23) : (f32, f32, f32) -> ()
      }) {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} : (tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>, tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) -> (tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>)
      "iree_tensor_ext.dispatch.tensor.store"(%17#2, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (tensor<20x4096x64xf32>, !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) -> ()
      "func.return"() : () -> ()
    }) : () -> ()
  }) : () -> ()
  "hal.executable.variant_end"() : () -> ()
}) : () -> ()
// -----// IR Dump After ConfigureTargetExecutableVariantsPass Failed (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @embedded_elf_x86_64 target(<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>) {
  hal.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
      %cst = arith.constant 0.000000e+00 : f32
      %cst_0 = arith.constant 1.250000e-01 : f32
      %c0 = arith.constant 0 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
      %5 = tensor.empty() : tensor<20x4096x4096xf32>
      %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
      %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
      ^bb0(%in: f16, %in_1: f16, %out: f32):
        %15 = arith.extf %in : f16 to f32
        %16 = arith.extf %in_1 : f16 to f32
        %17 = arith.mulf %15, %16 : f32
        %18 = arith.addf %17, %out : f32
        linalg.yield %18 : f32
      } -> tensor<20x4096x4096xf32>
      %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
      ^bb0(%in: f32, %out: f32):
        %15 = arith.mulf %in, %cst_0 : f32
        linalg.yield %15 : f32
      } -> tensor<20x4096x4096xf32>
      %9 = tensor.empty() : tensor<20x4096x64xf32>
      %10 = tensor.empty() : tensor<20x4096xf32>
      %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
      %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
      %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
      ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
        %15 = arith.addf %arg0, %arg3 : f32
        %16 = arith.truncf %arg0 : f32 to f16
        %17 = arith.extf %16 : f16 to f32
        %18 = arith.extf %arg1 : f16 to f32
        %19 = arith.mulf %17, %18 : f32
        %20 = arith.addf %19, %arg4 : f32
        iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
      } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
      iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
      return
    }
  }
}

./sandbox/dispatch.mlir:6:10: error: failed to configure executables
    %3 = flow.dispatch.workgroups(%0, %1, %2) : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) -> tensor<20x4096x64xf32> =
         ^
./sandbox/dispatch.mlir:6:10: note: see current operation:
"hal.executable"() <{sym_name = "attention_dispatch_0", sym_visibility = "private"}> ({
  "hal.executable.variant"() <{sym_name = "embedded_elf_x86_64", target = #hal.executable.target<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>}> ({
    "hal.executable.export"() <{layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, ordinal = 0 : index, sym_name = "attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32"}> ({
    ^bb0(%arg18: !hal.device):
      %29:3 = "iree_tensor_ext.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      "hal.return"(%29#0, %29#1, %29#2) : (index, index, index) -> ()
    }, {
    }) : () -> ()
    "builtin.module"() ({
      "func.func"() <{function_type = () -> (), sym_name = "attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32"}> ({
        %0 = "arith.constant"() <{value = 0.000000e+00 : f32}> : () -> f32
        %1 = "arith.constant"() <{value = 1.250000e-01 : f32}> : () -> f32
        %2 = "arith.constant"() <{value = 0 : index}> : () -> index
        %3 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %4 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %5 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %6 = "iree_tensor_ext.dispatch.tensor.load"(%3) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
        %7 = "iree_tensor_ext.dispatch.tensor.load"(%4) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>) -> tensor<20x4096x64xf16>
        %8 = "tensor.empty"() : () -> tensor<20x4096x4096xf32>
        %9 = "linalg.fill"(%0, %8) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg16: f32, %arg17: f32):
          "linalg.yield"(%arg16) : (f32) -> ()
        }) : (f32, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %10 = "linalg.generic"(%6, %7, %9) <{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 1>}> ({
        ^bb0(%arg13: f16, %arg14: f16, %arg15: f32):
          %25 = "arith.extf"(%arg13) : (f16) -> f32
          %26 = "arith.extf"(%arg14) : (f16) -> f32
          %27 = "arith.mulf"(%25, %26) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %28 = "arith.addf"(%27, %arg15) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%28) : (f32) -> ()
        }) {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} : (tensor<20x4096x64xf16>, tensor<20x4096x64xf16>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %11 = "linalg.generic"(%10, %8) <{indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = [#linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>, #linalg.iterator_type<parallel>], operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg11: f32, %arg12: f32):
          %24 = "arith.mulf"(%arg11, %1) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "linalg.yield"(%24) : (f32) -> ()
        }) : (tensor<20x4096x4096xf32>, tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %12 = "tensor.empty"() : () -> tensor<20x4096x64xf32>
        %13 = "tensor.empty"() : () -> tensor<20x4096xf32>
        %14 = "linalg.fill"(%1, %12) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg9: f32, %arg10: f32):
          "linalg.yield"(%arg9) : (f32) -> ()
        }) : (f32, tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %15 = "linalg.fill"(%0, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg7: f32, %arg8: f32):
          "linalg.yield"(%arg7) : (f32) -> ()
        }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %16 = "linalg.fill"(%1, %13) <{operandSegmentSizes = array<i32: 1, 1>}> ({
        ^bb0(%arg5: f32, %arg6: f32):
          "linalg.yield"(%arg5) : (f32) -> ()
        }) : (f32, tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %17:3 = "iree_linalg_ext.exp_reduction"(%11, %7, %15, %16, %14) <{exp_reduced_operands = array<i64: 1, 2>, indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], operandSegmentSizes = array<i32: 2, 3>}> ({
        ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
          %18 = "arith.addf"(%arg0, %arg3) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %19 = "arith.truncf"(%arg0) : (f32) -> f16
          %20 = "arith.extf"(%19) : (f16) -> f32
          %21 = "arith.extf"(%arg1) : (f16) -> f32
          %22 = "arith.mulf"(%20, %21) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          %23 = "arith.addf"(%22, %arg4) <{fastmath = #arith.fastmath<none>}> : (f32, f32) -> f32
          "iree_linalg_ext.yield"(%arg2, %18, %23) : (f32, f32, f32) -> ()
        }) {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} : (tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>, tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) -> (tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>)
        "iree_tensor_ext.dispatch.tensor.store"(%17#2, %5) <{operandSegmentSizes = array<i32: 1, 1, 0, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 20, 4096, 64>, static_strides = array<i64: 1, 1, 1>}> : (tensor<20x4096x64xf32>, !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>) -> ()
        "func.return"() : () -> ()
      }) : () -> ()
    }) : () -> ()
    "hal.executable.variant_end"() : () -> ()
  }) : () -> ()
  "hal.executable_end"() : () -> ()
}) : () -> ()
// -----// IR Dump After ConfigureExecutablesPass Failed (iree-hal-configure-executables) //----- //
hal.executable private @attention_dispatch_0 {
  hal.executable.variant public @embedded_elf_x86_64 target(<"llvm-cpu", "embedded-elf-x86_64", {cpu = "", cpu_features = "", data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", iree.encoding.resolver = #iree_cpu.cpu_encoding_resolver<>, max_stack_allocation_size = 32768 : i64, native_vector_size = 16 : i64, target_triple = "x86_64-unknown-unknown-eabi-elf"}>) {
    hal.executable.export public @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      hal.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @attention_dispatch_0_matmul_like_20x4096x4096x64_f16xf16xf32() {
        %cst = arith.constant 0.000000e+00 : f32
        %cst_0 = arith.constant 1.250000e-01 : f32
        %c0 = arith.constant 0 : index
        %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>>
        %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        %3 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %4 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<20x4096x64xf16>> -> tensor<20x4096x64xf16>
        %5 = tensor.empty() : tensor<20x4096x4096xf32>
        %6 = linalg.fill ins(%cst : f32) outs(%5 : tensor<20x4096x4096xf32>) -> tensor<20x4096x4096xf32>
        %7 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d2, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel", "reduction"]} ins(%3, %4 : tensor<20x4096x64xf16>, tensor<20x4096x64xf16>) outs(%6 : tensor<20x4096x4096xf32>) attrs =  {attention_qk_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0], vector_common_parallel = [0, 0, 0, 32]>} {
        ^bb0(%in: f16, %in_1: f16, %out: f32):
          %15 = arith.extf %in : f16 to f32
          %16 = arith.extf %in_1 : f16 to f32
          %17 = arith.mulf %15, %16 : f32
          %18 = arith.addf %17, %out : f32
          linalg.yield %18 : f32
        } -> tensor<20x4096x4096xf32>
        %8 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>], iterator_types = ["parallel", "parallel", "parallel"]} ins(%7 : tensor<20x4096x4096xf32>) outs(%5 : tensor<20x4096x4096xf32>) {
        ^bb0(%in: f32, %out: f32):
          %15 = arith.mulf %in, %cst_0 : f32
          linalg.yield %15 : f32
        } -> tensor<20x4096x4096xf32>
        %9 = tensor.empty() : tensor<20x4096x64xf32>
        %10 = tensor.empty() : tensor<20x4096xf32>
        %11 = linalg.fill ins(%cst_0 : f32) outs(%9 : tensor<20x4096x64xf32>) -> tensor<20x4096x64xf32>
        %12 = linalg.fill ins(%cst : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %13 = linalg.fill ins(%cst_0 : f32) outs(%10 : tensor<20x4096xf32>) -> tensor<20x4096xf32>
        %14:3 = iree_linalg_ext.exp_reduction{indexing_maps = [affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>, affine_map<(d0, d1, d2, d3) -> (d0, d3, d2)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1)>, affine_map<(d0, d1, d2, d3) -> (d0, d1, d2)>], iterator_types = [#iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<parallel>, #iree_linalg_ext.iterator_type<reduction>], exp_reduced_operands = [1, 2]} attributes {attention_pv_matmul, lowering_config = #iree_cpu.lowering_config<distribution = [32, 16, 0, 0]>} ins(%8, %4 : tensor<20x4096x4096xf32>, tensor<20x4096x64xf16>) outs(%12, %13, %11 : tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>) {
        ^bb0(%arg0: f32, %arg1: f16, %arg2: f32, %arg3: f32, %arg4: f32):
          %15 = arith.addf %arg0, %arg3 : f32
          %16 = arith.truncf %arg0 : f32 to f16
          %17 = arith.extf %16 : f16 to f32
          %18 = arith.extf %arg1 : f16 to f32
          %19 = arith.mulf %17, %18 : f32
          %20 = arith.addf %19, %arg4 : f32
          iree_linalg_ext.yield %arg2, %15, %20 : f32, f32, f32
        } -> tensor<20x4096xf32>, tensor<20x4096xf32>, tensor<20x4096x64xf32>
        iree_tensor_ext.dispatch.tensor.store %14#2, %2, offsets = [0, 0, 0], sizes = [20, 4096, 64], strides = [1, 1, 1] : tensor<20x4096x64xf32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<20x4096x64xf32>>
        return
      }
    }
  }
}
